{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNssfyuoAhdF2T+nDeyIUVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syannmi/Syannmi/blob/main/EX02_01_My_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAPAMfxXGoDk"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"\") #取得API後，輸入\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash-exp\") #模型\n",
        "response = model.generate_content(\"Explain how AI works\") #取得回應（提示詞）\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=userdata.get('Syannmi')) #從儲存的金鑰取的api key"
      ],
      "metadata": {
        "id": "Z9bCbQ9CQBS1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-2.0-flash-exp\") #模型\n",
        "response = model.generate_content(\"Explain how AI works\") #取得回應（提示詞）\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "YPB7yLt0QV6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    \"temperature\": 1, #代表適度的隨機性，平衡創意與穩定性\n",
        "    \"max_output_tokens\":1200 #設定 1200 代表模型最多輸出 1200 個 Token，以控制輸出長度。\n",
        "}\n",
        "model = genai.GenerativeModel( #初始化 Google 的生成式 AI 模型。\n",
        "    model_name=\"gemini-2.0-flash\",\n",
        "    generation_config=generation_config #把之前設定的生成配置（temperature 和 max_output_tokens）應用到這個模型中。\n",
        ")"
      ],
      "metadata": {
        "id": "BIZdJVP0RC3h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# 使用連續對話\n",
        "chat = model.start_chat(history=[]) #開始聊天\n",
        "\n",
        "while True: #連續迴圈\n",
        "  message = input('You:')\n",
        "  if message == '再見囉':\n",
        "    break #終止條件\n",
        "  response = chat.send_message(message) #送出訊息，回應\n",
        "  print('Bot:')\n",
        "  display(Markdown(response.text)) #顯示markdown語法\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uudtt3DySY6u",
        "outputId": "f0ad7b8f-3d3b-4cff-910a-6d8aac847e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You:hello\n",
            "Bot:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello! How can I help you today?\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You:could you tell me what large langague model is ?\n",
            "Bot:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a breakdown of what a large language model (LLM) is:\n\n**In a Nutshell:**\n\nA large language model (LLM) is a type of artificial intelligence (AI) model that is trained on a massive amount of text data to understand, generate, and manipulate human language. Think of it as a really smart computer program that has read countless books, articles, websites, and conversations, allowing it to learn the patterns and nuances of language.\n\n**Key Characteristics and Components:**\n\n*   **Large:**  The \"large\" in LLM refers to the massive size of the model.  This includes:\n    *   **Number of Parameters:**  Parameters are the internal variables within the model that are adjusted during training. LLMs typically have billions (or even trillions) of parameters. More parameters generally allow the model to learn more complex patterns.\n    *   **Training Data:**  LLMs are trained on incredibly vast datasets of text and code, often scraped from the internet, books, and other sources.  The quality and diversity of this data are crucial for the model's performance.\n\n*   **Language:** LLMs are specifically designed to work with human language.  They can:\n    *   **Understand Language:**  Analyze the meaning of text, identify entities, and understand the relationships between words and concepts.\n    *   **Generate Language:**  Create new text that is coherent, grammatically correct, and relevant to a given prompt or context.\n    *   **Translate Language:** Convert text from one language to another.\n    *   **Summarize Language:** Condense long pieces of text into shorter summaries.\n    *   **Answer Questions:** Provide answers to questions based on the information they have learned.\n\n*   **Model:**  LLMs are typically based on a type of neural network architecture called a **transformer**.  Transformers are particularly good at processing sequential data like text.  Key aspects of the transformer architecture that enable LLMs are:\n    *   **Attention Mechanism:**  Allows the model to focus on the most relevant parts of the input text when making predictions.\n    *   **Parallel Processing:**  Enables faster training compared to older recurrent neural network (RNN) architectures.\n\n**How They Work (Simplified):**\n\n1.  **Training:** The LLM is fed massive amounts of text data.  During training, the model learns to predict the next word in a sequence.  For example, if the input is \"The cat sat on the...\", the model learns to predict that the next word is likely \"mat.\"  The model adjusts its internal parameters to improve its prediction accuracy over time.  This process is repeated billions of times.\n\n2.  **Inference (Using the Model):** Once the model is trained, you can give it a prompt or question. The model uses its learned knowledge to generate a response.  It does this by predicting the most likely sequence of words that would follow the prompt.  The output is then presented as text.\n\n**Examples of LLMs:**\n\n*   **GPT (Generative Pre-trained Transformer) Family:**  Developed by OpenAI (e.g., GPT-3, GPT-4, ChatGPT)\n*   **LaMDA (Language Model for Dialogue Applications):** Developed by Google\n*   **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google\n*   **Llama (Large Language Model Meta AI):** Developed by Meta\n\n**Capabilities:**\n\nLLMs can perform a wide range of tasks, including:\n\n*   **Chatbots and Conversational AI:**  Powering virtual assistants and customer service bots.\n*   **Content Generation:**  Writing articles, blog posts, poems, scripts, and other creative content.\n*   **Code Generation:**  Writing computer code in various programming languages.\n*   **Machine Translation:**  Translating text between languages.\n*   **Text Summarization:**  Creating concise summaries of long documents.\n*   **Question Answering:**  Answering questions based on provided text or general knowledge.\n*   **Text Classification:**  Categorizing text into different topics or categories.\n*   **Sentiment Analysis:**  Determining the emotional tone of text.\n\n**Limitations and Challenges:**\n\n*   **Bias:** LLMs can inherit biases from the data they are trained on, leading to outputs that are unfair or discriminatory.\n*   **Hallucinations:** LLMs can sometimes generate false or nonsensical information.\n*   **Lack of True Understanding:**  LLMs are good at manipulating language but don't necessarily \"understand\" the meaning in the same way that humans do. They are pattern-matching machines.\n*   **Computational Cost:** Training and running LLMs require significant computing resources, which can be expensive and energy-intensive.\n*   **Ethical Concerns:**  The use of LLMs raises ethical concerns related to misinformation, job displacement, and the potential for misuse.\n\n**In summary, a large language model is a powerful AI tool that can understand, generate, and manipulate human language with remarkable fluency. They are transforming many industries, but it's important to be aware of their limitations and ethical considerations.**\n\nDo you have any other questions about large language models?  Perhaps you'd like to know about a specific aspect in more detail, such as the transformer architecture, training process, or ethical implications?\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}